<!DOCTYPE html>
<html>

<head>
<title> Yu Sun </title>
</head>

<body style="font-size: 125%;">

<img src="img/profile_compress.jpg" alt="Photo" height="300">

<h1>Yu Sun</h1>
<p>Email: ys646 [at] stanford.edu</p>

<!-- <p>My team at Stanford is always looking for collaborators to work on next-generation LLMs and large video models. Please email me if you are interested, especially if you have experience with systems optimization, e.g. programming in CUDA / Triton. I usually respond within 24h. </p> -->

<p>
My research focuses on an algorithmic framework called test-time training.
Its core idea is that each test instance defines its own learning problem, with its own target of generalization.
This is usually realized by training a different model on-the-fly for each test instance using self-supervision.
</p>

<p>
I am a postdoc at Stanford University, hosted by Carlos Guestrin, Tatsu Hashimoto, and Sanmi Koyejo.
I completed my PhD in EECS at UC Berkeley, advised by Alyosha Efros and Moritz Hardt.
During my undergrad at Cornell University, I worked with Kilian Weinberger.
</p>

<p>
For a complete list of papers, please see my 
<a href="https://scholar.google.com/citations?user=a7drwRMAAAAJ&hl=en&oi=ao">Google Scholar</a>.
</p>

<p>
* indicates equal contribution.
</p>
<p><b>Learning to (Learn at Test Time): RNNs with Expressive Hidden States</b><br>
Yu Sun*, Xinhao Li*, Karan Dalal*, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois,
Xinlei Chen†, Xiaolong Wang†, Sanmi Koyejo†, Tatsunori Hashimoto†, Carlos Guestrin†<br>
[<a href="https://arxiv.org/abs/2407.04620">paper</a>]
[<a href="https://github.com/test-time-training/ttt-lm-jax">JAX code</a>]
[<a href="https://github.com/test-time-training/ttt-lm-pytorch">PyTorch code</a>]
</p>

<p><b>Learning to (Learn at Test Time)</b><br>
Yu Sun*, Xinhao Li*, Karan Dalal, Chloe Hsu, Sanmi Koyejo, Carlos Guestrin, 
Xiaolong Wang, Tatsunori Hashimoto†, Xinlei Chen†<br>
[<a href="https://arxiv.org/abs/2310.13807">paper</a>]
[<a href="https://github.com/test-time-training/mttt">code</a>]
</p>

<p><b>Test-Time Training on Nearest Neighbors for Large Language Models</b><br>
Moritz Hardt, Yu Sun<br>
ICLR 2024<br>
[<a href="https://arxiv.org/abs/2305.18466">paper</a>]
[<a href="https://github.com/socialfoundations/tttlm">code</a>]
</p>

<p><b>Test-Time Training on Video Streams</b><br>
Renhao Wang*, Yu Sun*, Yossi Gandelsman, Xinlei Chen, Alexei A. Efros, Xiaolong Wang<br>
JMLR<br>
[<a href="https://arxiv.org/abs/2307.05014">paper</a>]
[<a href="https://video-ttt.github.io/">website</a>]
</p>

<p><b>Test-Time Training with Masked Autoencoders</b><br>
Yossi Gandelsman*, Yu Sun*, Xinlei Chen, Alexei A. Efros<br>
NeurIPS 2022<br>
[<a href="https://arxiv.org/abs/2209.07522">paper</a>]
[<a href="https://yossigandelsman.github.io/ttt_mae/index.html">website</a>]
<!-- [<a href="https://youtu.be/NbuWxmMco30">talk</a>] -->
</p>

<p><b>Test-Time Training with Self-Supervision for Generalization under Distribution Shifts</b><br>
Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, Moritz Hardt<br>
ICML 2020<br>
[<a href="https://arxiv.org/abs/1909.13231">paper</a>]
[<a href="ttt/home.html">website</a>]
[<a href="https://youtu.be/NbuWxmMco30">talk</a>]
</p>

<h3>Older Papers</h3>

<p>
* indicates alphabetical order.
</p>

<p><b>On Calibration of Modern Neural Networks</b><br>
Chuan Guo*, Geoff Pleiss*, Yu Sun*, Kilian Q. Weinberger<br>
ICML 2017<br> 
[<a href="https://arxiv.org/abs/1706.04599">paper</a>]
[<a href="https://github.com/gpleiss/temperature_scaling">code</a>]
</p>

<p><b>Deep Networks with Stochastic Depth</b><br>
Gao Huang*, Yu Sun*, Zhuang Liu, Daniel Sedra, Kilian Q. Weinberger<br>
ECCV 2016<br> 
[<a href="https://arxiv.org/abs/1603.09382">paper</a>]
[<a href="https://github.com/yueatsprograms/Stochastic_Depth">code</a>]
[<a href="https://videolectures.net/eccv2016_sun_deep_networks/">talk</a>]
</p>
</body>
</html>